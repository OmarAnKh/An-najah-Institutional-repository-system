{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3429abd9",
   "metadata": {},
   "source": [
    "## Why do we need the evaluation? \n",
    "to test whether our system ranks the right abstract near the top or not.\n",
    "\n",
    "### What is the measures we will use ?\n",
    "\n",
    "#### Retrieval accuracy\n",
    "Do the queries return the correct document?\n",
    "\n",
    "#### Ranking quality\n",
    "Where in the result list does the correct document appear?\n",
    "\n",
    "#### Semantic search vs lexical search\n",
    "Does your vector-based search outperform keyword search?\n",
    "\n",
    "#### Hybrid performance\n",
    "Does combining lexical + semantic improve ranking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98344673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Project root / import setup ---\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Walk up from `start` until a folder containing `src` is found.\"\"\"\n",
    "    current = start.resolve()\n",
    "    for p in [current, *current.parents]:\n",
    "        if (p / \"src\").exists():\n",
    "            return p\n",
    "    return current\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from global_config import global_config\n",
    "from src.opensearch.open_search_client import OpenSearchClient\n",
    "from src.services.article_search_service import ArticleSearchService\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_PATH = PROJECT_ROOT / \"src\" / \"evaluation\" / \"evaluation_queries.csv\"\n",
    "INDEX_NAME = global_config.index_name\n",
    "TOP_K = 10\n",
    "\n",
    "# --- Helper: load evaluation CSV ---\n",
    "\n",
    "def load_evaluation_queries(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load evaluation CSV and keep only rows that contain a non-empty query.\n",
    "\n",
    "    Expected columns: ['chunk_id', 'query', ...].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalise column names just in case\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if \"query\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'query' column\")\n",
    "    if \"chunk_id\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'chunk_id' column\")\n",
    "\n",
    "    mask = df[\"query\"].astype(str).str.strip().ne(\"\")\n",
    "    eval_df = df.loc[mask].copy()\n",
    "\n",
    "    # Ensure chunk_id is string so it matches OpenSearch _id\n",
    "    eval_df[\"chunk_id\"] = eval_df[\"chunk_id\"].astype(str)\n",
    "    return eval_df\n",
    "\n",
    "\n",
    "eval_set = load_evaluation_queries(CSV_PATH)\n",
    "len(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fca3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('أثر استخدام استراتيجية التدريس التبادلي في تحصيل الرياضيات والاتجاه نحوها لدى طالبات الصف الخامس في نابلس',\n",
       " '0',\n",
       " ['8abfdcc5-1931-4a98-a21e-41b60d1ccd3f_1',\n",
       "  '8abfdcc5-1931-4a98-a21e-41b60d1ccd3f_0',\n",
       "  'bb3bd957-fb08-44de-9671-670725ad8b60_0',\n",
       "  'bb3bd957-fb08-44de-9671-670725ad8b60_1',\n",
       "  'b7489514-a95f-4cb7-ba0b-9e489b813ac5_1',\n",
       "  'b7489514-a95f-4cb7-ba0b-9e489b813ac5_0',\n",
       "  'dfba577b-1f52-4aaf-a60d-a0ef61d69de7_1',\n",
       "  'a5d1c01c-f2e1-4ffc-9fc4-6ca75ba2529f_0',\n",
       "  '225b2bbf-8ee4-43e6-a9e8-29ec732695bb_0',\n",
       "  'dfba577b-1f52-4aaf-a60d-a0ef61d69de7_0'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- OpenSearch query helper ---\n",
    "\n",
    "\n",
    "def build_semantic_query(user_query: str, k: int) -> Dict:\n",
    "    \"\"\"Build the OpenSearch query body to match your index mapping.\n",
    "\n",
    "    Uses lexical search over title and abstract fields in both languages.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": user_query,\n",
    "                \"fields\": [\n",
    "                    \"title.en^3\",\n",
    "                    \"title.ar^3\",\n",
    "                    \"abstract.en^2\",\n",
    "                    \"abstract.ar^2\",\n",
    "                    \"author\",\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def run_search(\n",
    "    service: ArticleSearchService,\n",
    "    query_text: str,\n",
    "    k: int = TOP_K,\n",
    ") -> List[str]:\n",
    "    \"\"\"Run search and return a list of document IDs (as strings).\"\"\"\n",
    "    body = build_semantic_query(query_text, k)\n",
    "    resp = service.search_articles(body)\n",
    "    hits = resp.get(\"hits\", {}).get(\"hits\", [])\n",
    "    return [str(h.get(\"_id\")) for h in hits]\n",
    "\n",
    "\n",
    "# Create OpenSearch-backed search service instance\n",
    "client = OpenSearchClient()\n",
    "search_service = ArticleSearchService(index=INDEX_NAME, client=client)\n",
    "\n",
    "# Quick smoke test on the first query row (optional)\n",
    "first_row = eval_set.iloc[0]\n",
    "first_query = first_row[\"query\"]\n",
    "first_expected_id = first_row[\"chunk_id\"]\n",
    "first_results = run_search(search_service, first_query, k=TOP_K)\n",
    "first_query, first_expected_id, first_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d53b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-query results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>expected_id</th>\n",
       "      <th>hit_rank</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>62d2a2b1-533e-404b-900e-755542538751_0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>222b26aa-4b0f-49f4-ac76-3dd027db7b28_0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.430677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.315465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>6d2b92d1-dfa7-4d4a-9b4a-8b4e5eb00bd6_0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.289065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                             expected_id  hit_rank  recall@10  \\\n",
       "2         0  3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0       3.0        1.0   \n",
       "6         0  3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0       1.0        1.0   \n",
       "7         0  62d2a2b1-533e-404b-900e-755542538751_0       3.0        1.0   \n",
       "8         0  222b26aa-4b0f-49f4-ac76-3dd027db7b28_0       4.0        1.0   \n",
       "9         0  3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0       8.0        1.0   \n",
       "11        0  6d2b92d1-dfa7-4d4a-9b4a-8b4e5eb00bd6_0       1.0        1.0   \n",
       "13        0  3abe1e26-2454-4ef3-87c5-1eeda7724d5b_0      10.0        1.0   \n",
       "\n",
       "         MRR   nDCG@10  \n",
       "2   0.333333  0.500000  \n",
       "6   1.000000  1.000000  \n",
       "7   0.333333  0.500000  \n",
       "8   0.250000  0.430677  \n",
       "9   0.125000  0.315465  \n",
       "11  1.000000  1.000000  \n",
       "13  0.100000  0.289065  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate metrics:\n",
      "num_queries: 7.0000\n",
      "recall@10: 1.0000\n",
      "MRR: 0.4488\n",
      "nDCG@10: 0.5765\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "# --- Helpers to derive expected OpenSearch IDs using abstract text ---\n",
    "\n",
    "def _normalize_text(value) -> str:\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "\n",
    "def resolve_expected_doc_id(\n",
    "    row,\n",
    "    service: ArticleSearchService,\n",
    "    cache: Dict[int, str | None],\n",
    ") -> str | None:\n",
    "    \"\"\"Infer the expected OpenSearch _id for this evaluation row.\n",
    "\n",
    "    We don't have IDs in the CSV, so we:\n",
    "    1) Use the full abstract (en/ar) from the CSV as a query\n",
    "       against the indexed abstract fields.\n",
    "    2) Take the top hit and extract its bitstream_uuid from _id\n",
    "       (pattern: '{bitstream_uuid}_{chunk_id}').\n",
    "    3) Combine that bitstream_uuid with the CSV chunk_id to\n",
    "       form the expected _id for this row.\n",
    "    Results are cached per-row index to avoid repeated lookups.\n",
    "    \"\"\"\n",
    "    idx = row.name\n",
    "    if idx in cache:\n",
    "        return cache[idx]\n",
    "\n",
    "    abstract_en = _normalize_text(row.get(\"abstract_en\") if \"abstract_en\" in row else None)\n",
    "    abstract_ar = _normalize_text(row.get(\"abstract_ar\") if \"abstract_ar\" in row else None)\n",
    "    text = abstract_en or abstract_ar\n",
    "    if not text:\n",
    "        cache[idx] = None\n",
    "        return None\n",
    "\n",
    "    body = {\n",
    "        \"size\": 1,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": text,\n",
    "                \"fields\": [\"abstract.en^2\", \"abstract.ar^2\"],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    resp = service.search_articles(body)\n",
    "    hits = resp.get(\"hits\", {}).get(\"hits\", [])\n",
    "    if not hits:\n",
    "        cache[idx] = None\n",
    "        return None\n",
    "\n",
    "    top = hits[0]\n",
    "    raw_id = str(top.get(\"_id\", \"\"))\n",
    "\n",
    "    bit_uuid = None\n",
    "    # Prefer explicit field if present\n",
    "    source = top.get(\"_source\") or {}\n",
    "    if \"bitstream_uuid\" in source:\n",
    "        bit_uuid = str(source[\"bitstream_uuid\"])\n",
    "    # Fallback: infer from _id pattern '{uuid}_{chunk}'\n",
    "    if not bit_uuid and \"_\" in raw_id:\n",
    "        bit_uuid = raw_id.rsplit(\"_\", 1)[0]\n",
    "\n",
    "    if not bit_uuid:\n",
    "        cache[idx] = None\n",
    "        return None\n",
    "\n",
    "    chunk_idx = str(row[\"chunk_id\"])\n",
    "    expected_id = f\"{bit_uuid}_{chunk_idx}\"\n",
    "    cache[idx] = expected_id\n",
    "    return expected_id\n",
    "\n",
    "\n",
    "# --- Ranking metrics ---\n",
    "\n",
    "\n",
    "def recall_at_k(relevant: List[str], ranked_list: List[str], k: int) -> float:\n",
    "    \"\"\"Compute recall@k for a single query.\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    retrieved_at_k = ranked_list[:k]\n",
    "    hits = sum(1 for r in relevant if r in retrieved_at_k)\n",
    "    return hits / len(relevant)\n",
    "\n",
    "\n",
    "def reciprocal_rank(relevant: List[str], ranked_list: List[str]) -> float:\n",
    "    \"\"\"Compute reciprocal rank for a single query.\"\"\"\n",
    "    for i, doc_id in enumerate(ranked_list, start=1):\n",
    "        if doc_id in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevant: List[str], ranked_list: List[str], k: int) -> float:\n",
    "    \"\"\"Compute nDCG@k for a single query.\n",
    "\n",
    "    Here each relevant document has gain 1, non-relevant 0.\n",
    "    \"\"\"\n",
    "    rel_set = set(relevant)\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(ranked_list[:k], start=1):\n",
    "        if doc_id in rel_set:\n",
    "            dcg += 1.0 / log2(i + 1)\n",
    "\n",
    "    ideal_gains = [1.0] * min(len(rel_set), k)\n",
    "    if not ideal_gains:\n",
    "        return 0.0\n",
    "    idcg = sum(g / log2(i + 1) for i, g in enumerate(ideal_gains, start=1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_search(\n",
    "    df: pd.DataFrame,\n",
    "    service: ArticleSearchService,\n",
    "    k: int = TOP_K,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    \"\"\"Run evaluation over all queries in df.\n",
    "\n",
    "    df is expected to contain columns:\n",
    "      - 'chunk_id'\n",
    "      - 'query'\n",
    "      - 'abstract_ar'\n",
    "      - 'abstract_en'\n",
    "    Returns a per-query metrics DataFrame and an aggregate metrics dict.\n",
    "    \"\"\"\n",
    "    per_query_rows = []\n",
    "    expected_cache: Dict[int, str | None] = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        qtext = row[\"query\"]\n",
    "        expected_id = resolve_expected_doc_id(row, service, expected_cache)\n",
    "        relevant_ids = [expected_id] if expected_id else []\n",
    "\n",
    "        ranked_ids = run_search(service, qtext, k=k)\n",
    "\n",
    "        r_at_k = recall_at_k(relevant_ids, ranked_ids, k)\n",
    "        rr = reciprocal_rank(relevant_ids, ranked_ids)\n",
    "        ndcg = ndcg_at_k(relevant_ids, ranked_ids, k)\n",
    "\n",
    "        hit_rank = None\n",
    "        if expected_id:\n",
    "            hit_rank = next(\n",
    "                (i + 1 for i, did in enumerate(ranked_ids) if did == expected_id),\n",
    "                None,\n",
    "            )\n",
    "\n",
    "        per_query_rows.append(\n",
    "            {\n",
    "                \"chunk_id\": row[\"chunk_id\"],\n",
    "                \"query\": qtext,\n",
    "                \"expected_id\": expected_id,\n",
    "                \"hit_rank\": hit_rank,\n",
    "                \"recall@%d\" % k: r_at_k,\n",
    "                \"MRR\": rr,\n",
    "                \"nDCG@%d\" % k: ndcg,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    per_query_df = pd.DataFrame(per_query_rows)\n",
    "\n",
    "    # Drop rows where we couldn't resolve an expected_id or where hit_rank is NaN\n",
    "    per_query_df = per_query_df.dropna(subset=[\"expected_id\", \"hit_rank\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"num_queries\": len(per_query_df),\n",
    "        \"recall@%d\" % k: per_query_df[\"recall@%d\" % k].mean() if not per_query_df.empty else 0.0,\n",
    "        \"MRR\": per_query_df[\"MRR\"].mean() if not per_query_df.empty else 0.0,\n",
    "        \"nDCG@%d\" % k: per_query_df[\"nDCG@%d\" % k].mean() if not per_query_df.empty else 0.0,\n",
    "    }\n",
    "\n",
    "    return per_query_df, metrics\n",
    "\n",
    "\n",
    "def print_summary(per_query_df: pd.DataFrame, metrics: Dict[str, float], k: int = TOP_K) -> None:\n",
    "    \"\"\"Pretty-print per-query and aggregate evaluation results.\"\"\"\n",
    "    display_cols = [\n",
    "        \"chunk_id\",\n",
    "        \"expected_id\",\n",
    "        \"hit_rank\",\n",
    "        f\"recall@{k}\",\n",
    "        \"MRR\",\n",
    "        f\"nDCG@{k}\",\n",
    "    ]\n",
    "\n",
    "    print(\"Per-query results:\")\n",
    "    display(per_query_df[display_cols])\n",
    "\n",
    "    print(\"\\nAggregate metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.4f}\" if isinstance(value, (int, float)) else f\"{name}: {value}\")\n",
    "\n",
    "\n",
    "per_query_results, agg_metrics = evaluate_search(eval_set, search_service, k=TOP_K)\n",
    "print_summary(per_query_results, agg_metrics, k=TOP_K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
